<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Shilong Liu Homepage</title> <meta name="author" content="Shilong Liu"> <meta name="description" content="Shilong Liu's homepage. "> <meta name="keywords" content="academic, computer vision, object detection, artifical intellengence."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%89&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="http://lsl.zone/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="https://scholar.google.com/citations?user=nkSVY3MAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/Slongliu" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/shilong-liu-63766a15a" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/atasteoff" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Shilong Liu Homepage </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/me-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/me-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/me-1400.webp"></source> <img src="/assets/img/me.jpg?94037ed96efc8416bd10424e02dbbe7d" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="me.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>Hi! This is Shilong Liu (刘世隆). I am a <a href="https://seed.bytedance.com/en/topseed" rel="external nofollow noopener" target="_blank">Research Scientist</a> at <a href="https://seed.bytedance.com/en/" rel="external nofollow noopener" target="_blank">Bytedance Seed</a>. I am an incoming postdoc at <a href="https://ai.princeton.edu/ai-lab" rel="external nofollow noopener" target="_blank">Priceton University AI Lab</a>, working with <a href="https://ece.princeton.edu/people/mengdi-wang" rel="external nofollow noopener" target="_blank">Prof. Mengdi Wang</a>.</p> <p>I obtained my PhD from the <a href="http://www.cs.tsinghua.edu.cn/publish/csen/index.html" rel="external nofollow noopener" target="_blank">Department of Computer Science and Technology</a>, <a href="https://www.tsinghua.edu.cn/en/" rel="external nofollow noopener" target="_blank">Tsinghua University</a>, under the supervision of Prof. <a href="https://www.leizhang.org/" rel="external nofollow noopener" target="_blank">Lei Zhang</a>, Prof. <a href="https://www.suhangss.me/" rel="external nofollow noopener" target="_blank">Hang Su</a>, and Prof. <a href="https://ml.cs.tsinghua.edu.cn/~jun/index.shtml" rel="external nofollow noopener" target="_blank">Jun Zhu</a>. I got my bachelor’s degree from the <a href="http://www.ie.tsinghua.edu.cn/eng/" rel="external nofollow noopener" target="_blank">Department of Industrial Engineering</a>, <a href="https://www.tsinghua.edu.cn/en/" rel="external nofollow noopener" target="_blank">Tsinghua University</a> in 2020.</p> <p>During most of my PhD years, I had the opportunity to intern at the <a href="https://idea.edu.cn/" rel="external nofollow noopener" target="_blank">International Digital Economy Academy (IDEA)</a>, under the supervision of <a href="https://www.leizhang.org/" rel="external nofollow noopener" target="_blank">Prof. Lei Zhang</a>. I was an intern at Bytedance, NVIDIA, Shengshu-tech, and Microsoft Research, where I was fortunate to collaborate with talented researchers including <a href="https://liuguilin1225.github.io/" rel="external nofollow noopener" target="_blank">Guilin Liu</a>, <a href="https://chrisding.github.io/" rel="external nofollow noopener" target="_blank">Zhiding Yu</a>, <a href="https://chunyuan.li/" rel="external nofollow noopener" target="_blank">Chunyuan Li</a>, <a href="https://sites.google.com/site/hcheng2site" rel="external nofollow noopener" target="_blank">Hao Cheng</a>, and <a href="https://jwyang.github.io/" rel="external nofollow noopener" target="_blank">Jianwei Yang</a>.</p> <p>My previous work focused on computer vision, multi-modal learning, and LLM agents. Here are some of my representative works:</p> <ul> <li> <p>Visual Perception: We proposed a series of improvements on Detection Transformer including <a href="https://github.com/IDEA-Research/DAB-DETR" rel="external nofollow noopener" target="_blank">DAB-DETR</a><img src="https://img.shields.io/github/stars/IDEA-Research/DAB-DETR" alt="GitHub stars">, <a href="https://github.com/IDEA-Research/DN-DETR" rel="external nofollow noopener" target="_blank">DN-DETR</a><img src="https://img.shields.io/github/stars/IDEA-Research/DN-DETR" alt="GitHub stars">, <a href="https://github.com/IDEA-Research/DINO" rel="external nofollow noopener" target="_blank">DINO</a><img src="https://img.shields.io/github/stars/IDEA-Research/DINO" alt="GitHub stars">, <a href="https://github.com/IDEA-Research/MaskDINO" rel="external nofollow noopener" target="_blank">MaskDINO</a><img src="https://img.shields.io/github/stars/IDEA-Research/MaskDINO" alt="GitHub stars">, and <a href="https://github.com/IDEA-Research/Stable-DINO" rel="external nofollow noopener" target="_blank">Stable-DINO</a><img src="https://img.shields.io/github/stars/IDEA-Research/Stable-DINO" alt="GitHub stars">. Notably, <a href="https://github.com/IDEA-Research/DINO" rel="external nofollow noopener" target="_blank">DINO</a> was the <strong>FIRST</strong> DETR-like model to achieve state-of-the-art performance on the COCO object detection leaderboard.</p> </li> <li> <p>Open-world Visual Understanding/Multi-model learning: Our innovative models <a href="https://github.com/IDEA-Research/GroundingDINO" rel="external nofollow noopener" target="_blank">Grounding DINO</a><img src="https://img.shields.io/github/stars/IDEA-Research/GroundingDINO" alt="GitHub stars">, <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything" rel="external nofollow noopener" target="_blank">Grounded-SAM</a><img src="https://img.shields.io/github/stars/IDEA-Research/Grounded-Segment-Anything" alt="GitHub stars">, and <a href="https://deepdataspace.com/" rel="external nofollow noopener" target="_blank">Grounding-DINO-1.5/1.6/DINO-X</a> have made significant strides in this field. Grounding DINO has become one of the most popular open-world object detection models, enabling us to DETECT and SEGMENT ANYTHING.</p> </li> <li> <p>LLM Agents: We proposed <a href="https://arxiv.org/pdf/2505.20286" rel="external nofollow noopener" target="_blank">Alita</a>, a generalist agent that <strong>ranked 1st in GAIA benchmarks, outperforming OpenAI Deep Research</strong>. We introduced <a href="https://github.com/LLaVA-VL/LLaVA-Plus-Codebase" rel="external nofollow noopener" target="_blank">LLaVA-Plus</a>, which enhances multi-modal large language models with visual expertise, and <a href="https://github.com/camel-ai/crab" rel="external nofollow noopener" target="_blank">Crab</a>, a Python-based framework for building and benchmarking LLM agent environments. We also extended agents to specific fields like <a href="https://arxiv.org/abs/2311.10537" rel="external nofollow noopener" target="_blank">medical</a> and <a href="https://arxiv.org/abs/2505.20246" rel="external nofollow noopener" target="_blank">history</a>.</p> </li> </ul> <p>If you’re interested in related topics and would like to collaborate, feel free to reach out with my email: slongliu86 [AT] gmail.com. <em>(Note: The email liusl20 [AT] mails.tsinghua.edu.cn will be deprecated; please use the Gmail address instead.)</em></p> <p>Feel free to add me with my WeChat: SLONG_88 (please include a brief note about yourself when sending a request).</p> <table> <tbody> <tr> <td><a href="https://scholar.google.com/citations?user=nkSVY3MAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Google Scholar</a></td> <td><a href="https://github.com/SlongLiu" rel="external nofollow noopener" target="_blank">GitHub</a></td> <td><a href="https://twitter.com/atasteoff" rel="external nofollow noopener" target="_blank">Twitter</a></td> <td><a href="https://www.zhihu.com/people/3089892" rel="external nofollow noopener" target="_blank">Zhihu 知乎</a></td> <td><a href="https://drive.google.com/file/d/1oxb-vADJiA-spvOXSK-l66AwmGOK1Stc/view?usp=sharing" rel="external nofollow noopener" target="_blank">CV</a></td> </tr> </tbody> </table> </div> <h2><a href="/news/" style="color: inherit;">News</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Nov 11, 2024</th> <td> Invited talk at EECS 542, University of Michigan. [<a href="https://drive.google.com/file/d/1jO8CvhAnpHehUeoXgTO3kz5u9WSctaS7/view?usp=drive_link" rel="external nofollow noopener" target="_blank">Slides</a>] </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 22, 2024</th> <td> Start my internship at NVIDIA, collabrating with <a href="https://liuguilin1225.github.io/" rel="external nofollow noopener" target="_blank">Guilin Liu</a> and <a href="https://chrisding.github.io/" rel="external nofollow noopener" target="_blank">Zhiding Yu</a>. See you at the Bay Area, USA. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 4, 2024</th> <td> I was awarded as the <a href="https://www.thegaiaa.org/en/awards_mrzx#mrzx" rel="external nofollow noopener" target="_blank">2024 WAIC Yunfan Award – Rising Star</a>. [<a href="https://www.jiqizhixin.com/articles/2024-05-24-4" rel="external nofollow noopener" target="_blank">News</a>] </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 3, 2024</th> <td> 6 papers are accepted by ECCV 2024. See their details: <ul> <li><a href="https://arxiv.org/abs/2303.05499" rel="external nofollow noopener" target="_blank">Grounding DINO</a></li> <li><a href="https://llava-vl.github.io/llava-plus/" rel="external nofollow noopener" target="_blank">LLaVA-Plus</a></li> <li><a href="https://taptr.github.io/" rel="external nofollow noopener" target="_blank">TAPTR: Tracking Any Point</a></li> <li><a href="https://github.com/IDEA-Research/T-Rex" rel="external nofollow noopener" target="_blank">T-Rex2: Text-Visual Prompted Detector</a></li> <li><a href="https://llava-vl.github.io/llava-grounding/" rel="external nofollow noopener" target="_blank">LLaVA-Grounding</a></li> <li><a href="https://github.com/UX-Decoder/Semantic-SAM" rel="external nofollow noopener" target="_blank">Semantic-SAM</a></li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">May 17, 2024</th> <td> We introduce Grounding DINO 1.5, which is our most powerful open-world object detection model series. View our <a href="https://deepdataspace.com/blog/Grounding-DINO-1.5-Pro" rel="external nofollow noopener" target="_blank">blog</a> and <a href="https://arxiv.org/abs/2405.10300" rel="external nofollow noopener" target="_blank">tech report</a> for more details. <a href="https://deepdataspace.com/playground/grounding_dino" rel="external nofollow noopener" target="_blank">Try our demo</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 19, 2024</th> <td> Invited talk at <a href="https://cemse.kaust.edu.sa/ai/aii-symp-2024" rel="external nofollow noopener" target="_blank">Rising Stars in AI Symposium 2024 at KAUST</a>. I really enjoy the trip. </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 1, 2023</th> <td> <a href="https://github.com/IDEA-Research/DINO" rel="external nofollow noopener" target="_blank">DINO</a> and <a href="https://arxiv.org/abs/2201.12329" rel="external nofollow noopener" target="_blank">DAB-DETR</a> are awarded as the <a href="https://www.paperdigest.org/2023/09/most-influential-iclr-papers-2023-09/" rel="external nofollow noopener" target="_blank"><strong>most influential papers for ICLR 2023 and ICLR 2022</strong></a>, respectively. <a href="https://github.com/IDEA-Research/MaskDINO" rel="external nofollow noopener" target="_blank">Mask DINO</a> is selected as one of the <a href="https://www.paperdigest.org/2023/09/most-influential-iclr-papers-2023-09/" rel="external nofollow noopener" target="_blank"><strong>most influential paper for CVPR 2023</strong></a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 5, 2023</th> <td> I was awarded as the <a href="https://mp.weixin.qq.com/s/CYUMW82_ICCXv2isxLBM4g" rel="external nofollow noopener" target="_blank">CCF-CV Academic Emerging Scholar 2023</a> (CCF-CV 学术新锐学者, 3 people per year)! Thanks to the China Computer Federation. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 29, 2023</th> <td> Invited talks at <a href="https://air.tsinghua.edu.cn/en/" rel="external nofollow noopener" target="_blank">Institute for Al Industry Research (AIR), Tsinghua University</a>, <a href="https://ai.comp.nus.edu.sg/" rel="external nofollow noopener" target="_blank">HPC-AI Lab National University of Singapore</a>, <a href="http://ai.ruc.edu.cn/english/" rel="external nofollow noopener" target="_blank">Gaoling School of Artificial Intelligence</a> at <a href="https://en.ruc.edu.cn/" rel="external nofollow noopener" target="_blank">Renmin University of China (RUC)</a>, and <a href="http://valser.org/article-667-1.html" rel="external nofollow noopener" target="_blank">VALSE Student Webinar</a>. View the slides <a href="https://github.com/SlongLiu/mycv-v2/releases/download/slide/1124_detection_grounding_language.pdf" rel="external nofollow noopener" target="_blank">here (Slides about detection, grounding, and large language models)</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 13, 2023</th> <td> We release a strong open-set object detection model <a href="https://arxiv.org/abs/2303.05499" rel="external nofollow noopener" target="_blank"><strong>Grounding DINO</strong></a> that achieves the best results on open-set object detection tasks. It achieves <strong>52.5 zero-shot</strong> AP on COCO detection, without any COCO training data! It achieves <strong>63.0</strong> AP on COCO after fine-tuning. Code and checkpoints will be available <a href="https://github.com/IDEA-Research/GroundingDINO" rel="external nofollow noopener" target="_blank">here</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 22, 2022</th> <td> We release a toolbox <a href="https://github.com/IDEA-Research/detrex" rel="external nofollow noopener" target="_blank">detrex</a> that provides state-of-the-art Transformer-based detection algorithms. It includes DINO with better performance. Welcome to use it! </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">Selected Publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/gd15.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/gd15.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/gd15.gif-1400.webp"></source> <img src="/assets/img/publication_preview/gd15.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="gd15.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ren2024grounding" class="col-sm-8"> <div class="title">Grounding DINO 1.5: Advance the "Edge" of Open-Set Object Detection</div> <div class="author"> Tianhe Ren*, Qing Jiang*, <em>Shilong Liu*</em>, and <span class="more-authors" title="click to view 13 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '13 more authors' ? 'Zhaoyang Zeng*, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, Yuda Xiong, Hao Zhang, Feng Li, Peijun Tang, Kent Yu, Lei Zhang' : '13 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">13 more authors</span> </div> <div class="periodical"> <em>arXiv:2405.10300</em>, 2024 </div> <div class="periodical"> </div> <div class="summary"> Grounding DINO 1.5 Pro — our most capable model for open-set object detection. </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://deepdataspace.com/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ren2024grounding</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Grounding DINO 1.5: Advance the "Edge" of Open-Set Object Detection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ren*, Tianhe and Jiang*, Qing and Liu*, Shilong and Zeng*, Zhaoyang and Liu, Wenlong and Gao, Han and Huang, Hongjie and Ma, Zhengyu and Jiang, Xiaoke and Chen, Yihao and Xiong, Yuda and Zhang, Hao and Li, Feng and Tang, Peijun and Yu, Kent and Zhang, Lei}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:2405.10300}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2405.10300}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/llava-plus-pv-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/llava-plus-pv-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/llava-plus-pv-1400.webp"></source> <img src="/assets/img/publication_preview/llava-plus-pv.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="llava-plus-pv.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2023grounding" class="col-sm-8"> <div class="title">LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents</div> <div class="author"> <em>Shilong Liu</em>, Hao Cheng, Haotian Liu, and <span class="more-authors" title="click to view 10 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '10 more authors' ? 'Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang, Jianfeng Gao, Chunyuan Li' : '10 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">10 more authors</span> </div> <div class="periodical"> <em>To be shown in ECCV</em>, 2024 </div> <div class="periodical"> </div> <div class="summary"> Equip multimodal large language models with tools to create multimodal agents. </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://llava-vl.github.io/llava-plus/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/LLaVA-VL/LLaVA-Plus-Codebase" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-Plus-Codebase" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLaVA-VL/LLaVA-Plus-Codebase"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2023grounding</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Shilong and Cheng, Hao and Liu, Haotian and Zhang, Hao and Li, Feng and Ren, Tianhe and Zou, Xueyan and Yang, Jianwei and Su, Hang and Zhu, Jun and Zhang, Lei and Gao, Jianfeng and Li, Chunyuan}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{To be shown in ECCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">codebadge</span> <span class="p">=</span> <span class="s">{https://img.shields.io/github/stars/LLaVA-VL/LLaVA-Plus-Codebase}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/groundingdino_pv-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/groundingdino_pv-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/groundingdino_pv-1400.webp"></source> <img src="/assets/img/publication_preview/groundingdino_pv.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="groundingdino_pv.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2023groundinh" class="col-sm-8"> <div class="title">Grounding DINO: Marrying dino with grounded pre-training for open-set object detection</div> <div class="author"> <em>Shilong Liu</em>, Zhaoyang Zeng, Tianhe Ren, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>To be shown in ECCV</em>, 2023 </div> <div class="periodical"> </div> <div class="summary"> SOTA open-set object detector. 52.5AP on COCO without COCO training data! </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/IDEA-Research/GroundingDINO" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://img.shields.io/github/stars/IDEA-Research/GroundingDINO" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IDEA-Research/GroundingDINO"> </a> <a href="https://img.shields.io/github/stars/IDEA-Research/Grounded-Segment-Anything" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IDEA-Research/Grounded-Segment-Anything"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2023groundinh</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Grounding {DINO}: Marrying dino with grounded pre-training for open-set object detection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{To be shown in ECCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">codebadge</span> <span class="p">=</span> <span class="s">{https://img.shields.io/github/stars/IDEA-Research/GroundingDINO,https://img.shields.io/github/stars/IDEA-Research/Grounded-Segment-Anything}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/dino_pv-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/dino_pv-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/dino_pv-1400.webp"></source> <img src="/assets/img/publication_preview/dino_pv.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="dino_pv.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2022dino" class="col-sm-8"> <div class="title">DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection</div> <div class="author"> Hao Zhang*, Feng Li*, <em>Shilong Liu*</em>, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Lei Zhang, Hang Su, Jun Zhu, Lionel M. Ni, Heung-Yeung Shum' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>In International Conference on Learning Representations</em>, 2023 </div> <div class="periodical"> </div> <div class="summary"> The first DETR-based object detector that achieved 1st on the COCO detection leaderboard. </div> <div class="links"> <a href="http://arxiv.org/abs/2203.03605" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/IDEACVR/DINO" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://img.shields.io/github/stars/IDEA-Research/DINO" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IDEA-Research/DINO"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2022dino</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang*, Hao and Li*, Feng and Liu*, Shilong and Zhang, Lei and Su, Hang and Zhu, Jun and Ni, Lionel M. and Shum, Heung-Yeung}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">codebadge</span> <span class="p">=</span> <span class="s">{https://img.shields.io/github/stars/IDEA-Research/DINO}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/dabdetr_pv-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/dabdetr_pv-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/dabdetr_pv-1400.webp"></source> <img src="/assets/img/publication_preview/dabdetr_pv.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="dabdetr_pv.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2022dabdetr" class="col-sm-8"> <div class="title">DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR</div> <div class="author"> <em>Shilong Liu</em>, Feng Li, Hao Zhang, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, Lei Zhang' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>In International Conference on Learning Representations</em>, 2022 </div> <div class="periodical"> </div> <div class="summary"> A deep understanding of DETR’s query, and formulating queries as anchor boxes. </div> <div class="links"> <a href="http://arxiv.org/abs/2201.12329" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/IDEA-Research/DAB-DETR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://img.shields.io/github/stars/IDEA-Research/DAB-DETR" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IDEA-Research/DAB-DETR"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">liu2022dabdetr</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{DAB}-{DETR}: Dynamic Anchor Boxes are Better Queries for {DETR}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Shilong and Li, Feng and Zhang, Hao and Yang, Xiao and Qi, Xianbiao and Su, Hang and Zhu, Jun and Zhang, Lei}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=oMI9PjOb9Jl}</span><span class="p">,</span>
  <span class="na">codebadge</span> <span class="p">=</span> <span class="s">{https://img.shields.io/github/stars/IDEA-Research/DAB-DETR}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/q2l_pv-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/q2l_pv-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/q2l_pv-1400.webp"></source> <img src="/assets/img/publication_preview/q2l_pv.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="q2l_pv.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2021query2label" class="col-sm-8"> <div class="title">Query2Label: A Simple Transformer Way to Multi-Label Classification</div> <div class="author"> <em>Shilong Liu</em>, Lei Zhang, Xiao Yang, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Hang Su, Jun Zhu' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv:2107.10834</em>, 2021 </div> <div class="periodical"> </div> <div class="summary"> A novel transformer-based multi-label classification model, achieving SOTA on four benchmarks. </div> <div class="links"> <a href="http://arxiv.org/abs/2107.10834" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/SlongLiu/query2labels" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://img.shields.io/github/stars/SlongLiu/query2labels" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/SlongLiu/query2labels"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2021query2label</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Query2Label: A Simple Transformer Way to Multi-Label Classification}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Shilong and Zhang, Lei and Yang, Xiao and Su, Hang and Zhu, Jun}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv:2107.10834}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2107.10834}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CV}</span><span class="p">,</span>
  <span class="na">codebadge</span> <span class="p">=</span> <span class="s">{https://img.shields.io/github/stars/SlongLiu/query2labels}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="https://scholar.google.com/citations?user=nkSVY3MAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/Slongliu" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/shilong-liu-63766a15a" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/atasteoff" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2025 Shilong Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> Last updated: August 05, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>