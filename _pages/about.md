---
layout: about
title: about
permalink: /
# subtitle: PhD Candidate at <a href='https://www.tsinghua.edu.cn/en/'>Tsinghua University</a>. 

profile:
  align: right
  image: me.jpg
  image_circular: false # crops the image to make it circular
  # more_info: >
  #   <p>555 your office number</p>
  #   <p>123 your address street</p>
  #   <p>Your City, State 12345</p>

news: true  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---
Hi! This is Shilong Liu (刘世隆). I'm a `ceil(now() - 2020.9)`th year Ph.D. candidate at the [Department of Computer Science and Technology](http://www.cs.tsinghua.edu.cn/publish/csen/index.html), [Tsinghua University](https://www.tsinghua.edu.cn/en/), under the supervision of Prof. [Lei Zhang](https://www.leizhang.org/), Prof. [Hang Su](https://www.suhangss.me/), and Prof. [Jun Zhu](https://ml.cs.tsinghua.edu.cn/~jun/index.shtml). I got my bachelor's degree from the [Department of Industrial Engineering](http://www.ie.tsinghua.edu.cn/eng/), [Tsinghua University](https://www.tsinghua.edu.cn/en/) in 2020. 

I am a [Top Seed](https://seed.bytedance.com/en/topseed) intern at [Bytedance Seed](https://seed.bytedance.com/en/).

During most of my PhD years, I had the opportunity to intern at the  [International Digital Economy Academy (IDEA)](https://idea.edu.cn/), under the supervision of [Prof. Lei Zhang](https://www.leizhang.org/). I was an intern at NVIDIA and Microsoft Research, where I was fortunate to collaborate with talented researchers including [Guilin Liu](https://liuguilin1225.github.io/), [Zhiding Yu](https://chrisding.github.io/), [Chunyuan Li](https://chunyuan.li/), [Hao Cheng](https://sites.google.com/site/hcheng2site), and [Jianwei Yang](https://jwyang.github.io/).

My previous work focused on computer vision, multi-modal learning, and LLM agents. Here are some of my representative works:

- Visual Perception: We proposed a series of improvements on Detection Transformer including [DAB-DETR](https://github.com/IDEA-Research/DAB-DETR)<img src="https://img.shields.io/github/stars/IDEA-Research/DAB-DETR" alt="GitHub stars">, [DN-DETR](https://github.com/IDEA-Research/DN-DETR)<img src="https://img.shields.io/github/stars/IDEA-Research/DN-DETR" alt="GitHub stars">, [DINO](https://github.com/IDEA-Research/DINO)<img src="https://img.shields.io/github/stars/IDEA-Research/DINO" alt="GitHub stars">, [MaskDINO](https://github.com/IDEA-Research/MaskDINO)<img src="https://img.shields.io/github/stars/IDEA-Research/MaskDINO" alt="GitHub stars">, and [Stable-DINO](https://github.com/IDEA-Research/Stable-DINO)<img src="https://img.shields.io/github/stars/IDEA-Research/Stable-DINO" alt="GitHub stars">. Notably, [DINO](https://github.com/IDEA-Research/DINO) was the **FIRST** DETR-like model to achieve state-of-the-art performance on the COCO object detection leaderboard. 

- Open-world Visual Understanding/Multi-model learning: Our innovative models [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO)<img src="https://img.shields.io/github/stars/IDEA-Research/GroundingDINO" alt="GitHub stars">, [Grounded-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything)<img src="https://img.shields.io/github/stars/IDEA-Research/Grounded-Segment-Anything" alt="GitHub stars">, and [Grounding-DINO-1.5/1.6/DINO-X](https://deepdataspace.com/) have made significant strides in this field. Grounding DINO has become one of the most popular open-world object detection models, enabling us to DETECT and SEGMENT ANYTHING. 

- LLM Agents: We proposed [Alita](https://arxiv.org/pdf/2505.20286), a generalist agent that **ranked 1st in GAIA benchmarks, outperforming OpenAI Deep Research**. We introduced [LLaVA-Plus](https://github.com/LLaVA-VL/LLaVA-Plus-Codebase), which enhances multi-modal large language models with visual expertise, and [Crab](https://github.com/camel-ai/crab), a Python-based framework for building and benchmarking LLM agent environments. We also extended agents to specific fields like [medical](https://arxiv.org/abs/2311.10537) and [history](https://arxiv.org/abs/2505.20246).


If you're interested in related topics and would like to collaborate, feel free to reach out with my email: slongliu86 [AT] gmail.com. *(Note: The email liusl20 [AT] mails.tsinghua.edu.cn will be deprecated; please use the Gmail address instead.)*

Feel free to add me with my WeChat: SLONG_88 (please include a brief note about yourself when sending a request).

[Google Scholar](https://scholar.google.com/citations?user=nkSVY3MAAAAJ&hl=en) | [GitHub](https://github.com/SlongLiu) | [Twitter](https://twitter.com/atasteoff) | [Zhihu 知乎](https://www.zhihu.com/people/3089892) | [CV](https://drive.google.com/file/d/1oxb-vADJiA-spvOXSK-l66AwmGOK1Stc/view?usp=sharing) 


