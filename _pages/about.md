---
layout: about
title: about
permalink: /
# subtitle: PhD Candidate at <a href='https://www.tsinghua.edu.cn/en/'>Tsinghua University</a>. 

profile:
  align: right
  image: me.jpg
  image_circular: false # crops the image to make it circular
  # more_info: >
  #   <p>555 your office number</p>
  #   <p>123 your address street</p>
  #   <p>Your City, State 12345</p>

news: true  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---
Hi! This is Shilong Liu (Âàò‰∏ñÈöÜ). I am a **Postdoctoral Research Fellow** at the [Princeton AI Lab](https://ai.princeton.edu/ai-lab), [Princeton University](https://www.princeton.edu/), working with [Prof. Mengdi Wang](https://ece.princeton.edu/people/mengdi-wang). I obtained my Ph.D. from the [Department of Computer Science and Technology](http://www.cs.tsinghua.edu.cn/publish/csen/index.html), [Tsinghua University](https://www.tsinghua.edu.cn/en/), under the supervision of Prof. [Lei Zhang](https://www.leizhang.org/), Prof. [Hang Su](https://www.suhangss.me/), and Prof. [Jun Zhu](https://ml.cs.tsinghua.edu.cn/~jun/index.shtml). I received my B.Eng. degree from the [Department of Industrial Engineering](http://www.ie.tsinghua.edu.cn/eng/), Tsinghua University, in 2020.

Before joining Princeton, I was a **Research Scientist** at [Bytedance Seed](https://seed.bytedance.com/en/).

During my Ph.D. and research career, I have had the privilege to intern and collaborate at leading research labs, including [Bytedance](https://seed.bytedance.com/en/), [NVIDIA Research](https://research.nvidia.com/), [Microsoft Research Redmond](https://www.microsoft.com/en-us/research/lab/microsoft-research-redmond/), [IDEA Research](https://idea.edu.cn/), and [Shengshu Tech](https://shengshutech.com/), working with amazing mentors such as [Guilin Liu](https://liuguilin1225.github.io/), [Zhiding Yu](https://chrisding.github.io/), [Chunyuan Li](https://chunyuan.li/), [Hao Cheng](https://sites.google.com/site/hcheng2site), and [Jianwei Yang](https://jwyang.github.io/).

---

### üéØ Research Interests
My research lies at the intersection of **LLM Agents**, **Multimodal Learning**, **Computer Vision**, and **Physical Intelligence**. I am broadly interested in building **vision-language-action systems** that can *see, reason, and act* in open environments.

---

### üî¨ Representative Works

- **Visual Perception & DETR Evolution**  
  We introduced a series of Transformer-based detection models including [DAB-DETR](https://github.com/IDEA-Research/DAB-DETR) <img src="https://img.shields.io/github/stars/IDEA-Research/DAB-DETR" alt="GitHub stars">, [DN-DETR](https://github.com/IDEA-Research/DN-DETR) <img src="https://img.shields.io/github/stars/IDEA-Research/DN-DETR" alt="GitHub stars">, [DINO](https://github.com/IDEA-Research/DINO) <img src="https://img.shields.io/github/stars/IDEA-Research/DINO" alt="GitHub stars">, [MaskDINO](https://github.com/IDEA-Research/MaskDINO) <img src="https://img.shields.io/github/stars/IDEA-Research/MaskDINO" alt="GitHub stars">, and [Stable-DINO](https://github.com/IDEA-Research/Stable-DINO) <img src="https://img.shields.io/github/stars/IDEA-Research/Stable-DINO" alt="GitHub stars">. **DINO** was the *first DETR-like model* to achieve state-of-the-art performance on the COCO object detection leaderboard.

- **Open-world Visual Understanding & Multimodal Models**  
  We developed [Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) <img src="https://img.shields.io/github/stars/IDEA-Research/GroundingDINO" alt="GitHub stars"> and [Grounded-SAM](https://github.com/IDEA-Research/Grounded-Segment-Anything) <img src="https://img.shields.io/github/stars/IDEA-Research/Grounded-Segment-Anything" alt="GitHub stars">, empowering models to **detect and segment anything**. Grounding DINO is now the *most downloaded zero-shot object detection model on Hugging Face* and receives **over 2 million downloads per month**. The subsequent series‚ÄîGrounding-DINO-1.5, 1.6, and DINO-X‚Äîcontinues to push open-world perception forward.

- **LLM Agents & Generalist Intelligence**  
  We proposed [Alita](https://arxiv.org/pdf/2505.20286), a generalist agent that **ranked 1st on the GAIA benchmark**, surpassing OpenAI Deep Research. We also introduced [LLaVA-Plus](https://github.com/LLaVA-VL/LLaVA-Plus-Codebase), enhancing multimodal large language models with *vision-expert tool usage*, and [Crab](https://github.com/camel-ai/crab), a Python-based framework for *agent environment simulation and benchmarking*. Our recent work extends agents into specialized domains such as [medical reasoning](https://arxiv.org/abs/2311.10537) and [history cognition](https://arxiv.org/abs/2505.20246).

---

### üèÜ Awards & Recognitions
- **WAIC Yunfan Award ‚Äì Rising Star**, 2024 (15 people/year)  
- **KAUST AI Rising Star**, 2024 (Top 15%)  
- **CCF-CV Academic Emerging Scholar Award**, 2023 (3 people/year)  
- **Innovation 84 Scholarship**, 2024. 

---

### üìà Impact
- 11,000+ Google Scholar citations  
- 30,000+ GitHub stars  
- 4 papers selected as *Top 15 Most Influential Papers* by Paper Digest  

---

If you are interested in **multimodal agents** or **open-world vision models**, feel free to reach out at:  
üìß slongliu86 [AT] gmail.com or sl8264 [AT] princeton.edu  
*(Note: the Tsinghua email is deprecated; please use Gmail or Princeton email instead.)*

Feel free to add me on **WeChat: SLONG_88** (please include a short self-introduction).

[Google Scholar](https://scholar.google.com/citations?user=nkSVY3MAAAAJ&hl=en) | [GitHub](https://github.com/SlongLiu) | [LinkedIn](https://www.linkedin.com/in/shilong-liu/) | [Twitter](https://twitter.com/atasteoff) | [Zhihu Áü•‰πé](https://www.zhihu.com/people/3089892) | [CV (Google Drive)](https://drive.google.com/file/d/1oxb-vADJiA-spvOXSK-l66AwmGOK1Stc/view?usp=sharing)



